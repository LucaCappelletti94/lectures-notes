\providecommand{\main}{..}
\documentclass[\main/main.tex]{subfiles}
\begin{document}

Il sistema considerato è caratterizzato da \textbf{due attori}, l'ambiente e l'agente. L'agente modifica le proprie azioni in base alle reazioni dell'ambiente e questo comportamento adottato è diretto alla massimizzazione di una certa fitness. L'agente cerca di trovare una \textbf{policy}, cioè l'insieme delle azioni che in ogni istante massimizzano la \textbf{reward}. Lo stato dell'ambiente non cambia sino a che non viene effettuata un'azione (che in questo caso consideriamo le azioni come unicamente prodotte dall`agente). Le azioni esterne possono essere modellizzate o come ulteriori agente o come \textbf{interferenze esterne} o \textbf{rumore}.

\subsection{Value Function}
Si tratta del \textbf{reward a lungo termine} (Figura \ref{formula:value_function}) legato ad una determinata strategia di interazioni con l'ambiente, ed è legata ad una determinata policy $\pi$.

\begin{figure}
  \[
    V^\pi (S) = \sum_t^\infty R_t
  \]
  \caption{Value Function}
  \label{formula:value_function}
\end{figure}

\subsection{Rappresentazione delle azioni}
Il set delle azioni può essere rappresentato tramite un grafo a stati finiti (STG, state transition graph) che considera solitamente lo stato ad alta energia. Un automa solitamente o si muove verso lo stato a energia più bassa con una determinata probabilità o verso lo stato a energia più alta verso lo stato a energia più alta.

Una volta raggiunto lo stato a low energy, solitamente o si va a ricaricare o sta fermo.

\subsection{Reward a lungo termine}
Questo valore è pari al \textbf{valore atteso} della somma di tutti i reward da $0$ a $\infty$ per un determinato valore $\gamma$.
\begin{figure}
  \[
    E^\pi \left[ \sum_t^\infty \gamma_t R_t \right]
  \]
  \caption{Reward a lungo termine}
\end{figure}

\end{document}
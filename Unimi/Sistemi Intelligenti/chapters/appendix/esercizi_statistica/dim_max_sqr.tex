\providecommand{\main}{../../../}
\documentclass[\main/main.tex]{subfiles}
\begin{document}
\subsection{Dimostrare che la stima ai minimi quadrati è equivalente alla stima a massima verosimiglianza nel caso di errore Gaussiano sui dati. Cosa fornisce? Come? [3]}
Scriviamo il logaritmo negativo della verosimiglianza:

\begin{align*}
  \prob{y_1,...,y_n; n, b; x_1, ..., x_n } & =
  - \sum_{i=1}^n
  \ln \crl{
    \frac{1}{\sqrt{2\pi}\sigma}
    \exp \sqr{
      -\frac{1}{2} \rnd{\frac{ y_i - mx_i - b}{\sigma} }^2
    }
  }                                            \\
                                           & =
  - \sum_{i=1}^n \ln
  \rnd{ \frac{1}{\sqrt{2\pi}\sigma} }
  - \sum_{i=1}^n
  \sqr{
    -\frac{1}{2} \rnd{\frac{ y_i - mx_i - b}{\sigma} }^2
  }                                            \\
                                           & =
  - \sum_{i=1}^n \ln
  \rnd{ \frac{1}{\sqrt{2\pi}\sigma} }
  + \frac{1}{2\sigma^2} \sum_{i=1}^n
  \rnd{y_i - mx_i - b }^2
\end{align*}

Massimizziamo la \textbf{likelyhood} ponendo a zero le derivate prime rispetto a $m$:

\begin{align*}
  \frac{\partial \prob{y_1,...,y_n; n, b; x_1, ..., x_n }}{\partial m} & = \frac{\partial}{\partial m} \sqr{ - \sum_{i=1}^n \ln
    \rnd{ \frac{1}{\sqrt{2\pi}\sigma} }
    + \frac{1}{2\sigma^2} \sum_{i=1}^n
    \rnd{y_i - mx_i - b }^2
  }                                                                                                                                              \\
                                                                       & = 0 + \frac{1}{2\sigma^2} \sum_{i=1}^n \rnd{y_i - mx_i - b }^2 2 (-x_i) \\
                                                                       & = -\frac{1}{\sigma^2} \sum_{i=1}^n \rnd{y_i - mx_i - b }^2  x_i
\end{align*}

\[
  -\frac{1}{\sigma^2} \sum_{i=1}^n \rnd{y_i - mx_i - b }^2  x_i = 0
\]

\[
  \sum_{i=1}^n \rnd{y_i - mx_i - b }^2  x_i = 0
\]

\begin{figure}[H]
  \[
    m\sqr{ \sum_{i=1}^n \rnd{x_i^2 } } + q \sqr{ \sum_{i=1}^n \rnd{x_i } } = \sqr{ \sum_{i=1}^n \rnd{y_i x_i } }
  \]
  \caption{Prima equazione}
\end{figure}

Massimizziamo la \textbf{likelyhood} ponendo a zero le derivate prime rispetto a $q$:

\begin{align*}
  \frac{\partial \prob{y_1,...,y_n; n, b; x_1, ..., x_n }}{\partial q} & = \frac{\partial}{\partial q} \sqr{ - \sum_{i=1}^n \ln
    \rnd{ \frac{1}{\sqrt{2\pi}\sigma} }
    + \frac{1}{2\sigma^2} \sum_{i=1}^n
    \rnd{y_i - mx_i - b }^2
  }                                                                                                                                            \\
                                                                       & = 0 + \frac{1}{2\sigma^2} \sum_{i=1}^n \rnd{y_i - mx_i - b }^2 2 (-1) \\
                                                                       & = \frac{1}{\sigma^2} \sum_{i=1}^n \rnd{y_i - mx_i - b }^2
\end{align*}

\[
  \frac{1}{\sigma^2} \sum_{i=1}^n \rnd{y_i - mx_i - b }^2  = 0
\]

\[
  \sum_{i=1}^n \rnd{y_i - mx_i - b }^2 = 0
\]

\begin{figure}[H]
  \[
    m\sqr{ \sum_{i=1}^n \rnd{x_i } } + q \sqr{ \sum_{i=1}^n \rnd{1 } } = \sqr{ \sum_{i=1}^n \rnd{y_i } }
  \]
  \caption{Seconda equazione}
\end{figure}

Ponendo a sistema le equazioni così ottenute ottengo:
\[
  \begin{bmatrix}
    \sum_{i=1}^n \rnd{x_i^2 } & \sum_{i=1}^n \rnd{x_i } \\
    \sum_{i=1}^n \rnd{x_i }   & n
  \end{bmatrix}
  \begin{bmatrix}
    m \\
    b
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sum_{i=1}^n \rnd{y_i x_i } \\
    \sum_{i=1}^n \rnd{y_i }
  \end{bmatrix}
\]

Lo stesso problema visto dal punto di vista dei \textbf{minimi quadrati} è impostato nel seguente modo.

\[
  \begin{bmatrix}
    x_1    & 1      \\
    \vdots & \vdots \\
    x_n    & n
  \end{bmatrix}
  \begin{bmatrix}
    m \\
    b
  \end{bmatrix}
  =
  \begin{bmatrix}
    y_1    \\
    \vdots \\
    y_n
  \end{bmatrix}
\]

L'obbiettivo è trovare una $x$ tale che $(Ax-b)^T(Ax-b)$ è minima (minimizzazione di residui).
La soluzione si ottiene calcolando $A^TAx = A^Tb$.

\[
  A^TA = \begin{bmatrix}
    x_1 & \dots & x_n \\
    1   & \dots & n
  \end{bmatrix}
  \begin{bmatrix}
    x_1    & 1      \\
    \vdots & \vdots \\
    x_n    & n
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sum_{i=1}^n \rnd{x_i^2 } & \sum_{i=1}^n \rnd{x_i } \\
    \sum_{i=1}^n \rnd{x_i }   & n
  \end{bmatrix}
\]

\[
  A^Tb = \begin{bmatrix}
    x_1 & \dots & x_n \\
    1   & \dots & n
  \end{bmatrix}
  \begin{bmatrix}
    y_1    \\
    \vdots \\
    y_n
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sum_{i=1}^n \rnd{y_i x_i } \\
    \sum_{i=1}^n \rnd{y_i }
  \end{bmatrix}
\]

Ovvero:

\[
  \begin{bmatrix}
    \sum_{i=1}^n \rnd{x_i^2 } & \sum_{i=1}^n \rnd{x_i } \\
    \sum_{i=1}^n \rnd{x_i }   & n
  \end{bmatrix}
  \begin{bmatrix}
    m \\
    b
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sum_{i=1}^n \rnd{y_i x_i } \\
    \sum_{i=1}^n \rnd{y_i }
  \end{bmatrix}
\]

Che è la stessa soluzione ottenuta per la stima a massima verosimiglianza.

Queste metodologie offrono una stima dei parametri di una funzione tramite la minimizzazione dei residui.

La soluzione è quella che minimizza lo scarto quadratico medio dei residui, ovvero è a minima varianza.

\end{document}
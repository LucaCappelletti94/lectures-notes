\providecommand{\main}{../../}
\documentclass[\main/main.tex]{subfiles}
\begin{document}
\section{Domande su Statistica}
\subfile{\main/chapters/appendix/esercizi_statistica/taxi}
\subfile{\main/chapters/appendix/esercizi_statistica/tumore_seno}
\subfile{\main/chapters/appendix/esercizi_statistica/macchine}


\subsection{Enunciare il teorema di Bayes}
\bayesTh

\subsection{Discutere l'analisi di varianza per un sistema lineare [4]}
Svolgere un'analisi di varianza per un sistema lineare significa analizzare quanto la stima di un parametro possa variare nelle diverse misure dei dati relativi al problema.
L'analisi consente di esaminare a matrice dei covarianti, misurare quanto varia una misura di una variabile al variare del rumore e misurare quanto covariano due misure di due variabilità.
\textbf{L'indice di correlazione} di due variabili viene calcolato proprio per misurare quanto le variabili si trovino lungo una funzione.

\subsection{Dimostrare che la stima ai minimi quadrati è equivalente alla stima a massima verosimiglianza nel caso di errore Gaussiano sui dati. Cosa fornisce? Come? [3]}
Scriviamo il logaritmo negativo della verosimiglianza:

\begin{align}
	P \left(y_1,...,y_n; n, b; x_1, ..., x_n \right) & =
	- \sum_{i=1}^n
	\ln \left\{
	\dfrac{1}{\sqrt{2\pi}\sigma}
	\exp \left[
		-\dfrac{1}{2}	\left(\dfrac{ y_i - mx_i - b}{\sigma} \right)^2
		\right]
	\right\}                                             \\
	                                                 & =
	- \sum_{i=1}^n \ln
	\left ( \dfrac{1}{\sqrt{2\pi}\sigma} \right )
	- \sum_{i=1}^n
	\left[
		-\dfrac{1}{2}	\left(\dfrac{ y_i - mx_i - b}{\sigma} \right)^2
		\right]                                              \\
	                                                 & =
	- \sum_{i=1}^n \ln
	\left ( \dfrac{1}{\sqrt{2\pi}\sigma} \right )
	+ \dfrac{1}{2\sigma^2} \sum_{i=1}^n
	\left(y_i - mx_i - b \right)^2
\end{align}

Massimizziamo la \textbf{likelyhood} ponendo a zero le derivate prime rispetto a $m$:

\begin{align}
	\dfrac{\partial P \left(y_1,...,y_n; n, b; x_1, ..., x_n \right)}{\partial m} & = \dfrac{\partial}{\partial m} \left [ - \sum_{i=1}^n \ln
		\left ( \dfrac{1}{\sqrt{2\pi}\sigma} \right )
		+ \dfrac{1}{2\sigma^2} \sum_{i=1}^n
		\left(y_i - mx_i - b \right)^2
		\right ]                                                                                                                                                        \\
	                                                                              & = 0 + \dfrac{1}{2\sigma^2} \sum_{i=1}^n \left(y_i - mx_i - b \right)^2 2 (-x_i) \\
	                                                                              & = -\dfrac{1}{\sigma^2} \sum_{i=1}^n \left(y_i - mx_i - b \right)^2  x_i
\end{align}

\[
	-\dfrac{1}{\sigma^2} \sum_{i=1}^n \left(y_i - mx_i - b \right)^2  x_i = 0
\]

\[
	\sum_{i=1}^n \left(y_i - mx_i - b \right)^2  x_i = 0
\]

\begin{figure}[H]
	\[
		m\left [ \sum_{i=1}^n \left(x_i^2 \right) \right ] + q \left [ \sum_{i=1}^n \left(x_i \right) \right ] = \left [ \sum_{i=1}^n \left(y_i x_i \right) \right ]
	\]
	\caption{Prima equazione}
\end{figure}

Massimizziamo la \textbf{likelyhood} ponendo a zero le derivate prime rispetto a $q$:

\begin{align}
	\dfrac{\partial P \left(y_1,...,y_n; n, b; x_1, ..., x_n \right)}{\partial q} & = \dfrac{\partial}{\partial q} \left [ - \sum_{i=1}^n \ln
		\left ( \dfrac{1}{\sqrt{2\pi}\sigma} \right )
		+ \dfrac{1}{2\sigma^2} \sum_{i=1}^n
		\left(y_i - mx_i - b \right)^2
		\right ]                                                                                                                                                      \\
	                                                                              & = 0 + \dfrac{1}{2\sigma^2} \sum_{i=1}^n \left(y_i - mx_i - b \right)^2 2 (-1) \\
	                                                                              & = \dfrac{1}{\sigma^2} \sum_{i=1}^n \left(y_i - mx_i - b \right)^2
\end{align}

\[
	\dfrac{1}{\sigma^2} \sum_{i=1}^n \left(y_i - mx_i - b \right)^2  = 0
\]

\[
	\sum_{i=1}^n \left(y_i - mx_i - b \right)^2 = 0
\]

\begin{figure}[H]
	\[
		m\left [ \sum_{i=1}^n \left(x_i \right) \right ] + q \left [ \sum_{i=1}^n \left(1 \right) \right ] = \left [ \sum_{i=1}^n \left(y_i \right) \right ]
	\]
	\caption{Seconda equazione}
\end{figure}

Ponendo a sistema le equazioni così ottenute ottengo:
\[
	\begin{bmatrix}
		\left [ \sum_{i=1}^n \left(x_i^2 \right) \right ] & \left [ \sum_{i=1}^n \left(x_i \right) \right ] \\
		\left [ \sum_{i=1}^n \left(x_i \right) \right ]   & n
	\end{bmatrix}
	\begin{bmatrix}
		m \\
		b
	\end{bmatrix}
	=
	\begin{bmatrix}
		\left [ \sum_{i=1}^n \left(y_i x_i \right) \right ] \\
		\left [ \sum_{i=1}^n \left(y_i \right) \right ]
	\end{bmatrix}
\]

Lo stesso problema visto dal punto di vista dei \textbf{minimi quadrati} è impostato nel seguente modo.

\[
	\begin{bmatrix}
		x_1    & 1      \\
		\vdots & \vdots \\
		x_n    & n
	\end{bmatrix}
	\begin{bmatrix}
		m \\
		b
	\end{bmatrix}
	=
	\begin{bmatrix}
		y_1    \\
		\vdots \\
		y_n
	\end{bmatrix}
\]

L'obbiettivo è trovare una $x$ tale che $(Ax-b)^T(Ax-b)$ è minima (minimizzazione di residui).
La soluzione si ottiene calcolando $A^TAx = A^Tb$.

\[
	A^TA = \begin{bmatrix}
		x_1 & \dots & x_n \\
		1   & \dots & n
	\end{bmatrix}
	\begin{bmatrix}
		x_1    & 1      \\
		\vdots & \vdots \\
		x_n    & n
	\end{bmatrix}
	=
	\begin{bmatrix}
		\left [ \sum_{i=1}^n \left(x_i^2 \right) \right ] & \left [ \sum_{i=1}^n \left(x_i \right) \right ] \\
		\left [ \sum_{i=1}^n \left(x_i \right) \right ]   & n
	\end{bmatrix}
\]

\[
	A^Tb = \begin{bmatrix}
		x_1 & \dots & x_n \\
		1   & \dots & n
	\end{bmatrix}
	\begin{bmatrix}
		y_1    \\
		\vdots \\
		y_n
	\end{bmatrix}
	=
	\begin{bmatrix}
		\left [ \sum_{i=1}^n \left(y_i x_i \right) \right ] \\
		\left [ \sum_{i=1}^n \left(y_i \right) \right ]
	\end{bmatrix}
\]

Ovvero:

\[
	\begin{bmatrix}
		\left [ \sum_{i=1}^n \left(x_i^2 \right) \right ] & \left [ \sum_{i=1}^n \left(x_i \right) \right ] \\
		\left [ \sum_{i=1}^n \left(x_i \right) \right ]   & n
	\end{bmatrix}
	\begin{bmatrix}
		m \\
		b
	\end{bmatrix}
	=
	\begin{bmatrix}
		\left [ \sum_{i=1}^n \left(y_i x_i \right) \right ] \\
		\left [ \sum_{i=1}^n \left(y_i \right) \right ]
	\end{bmatrix}
\]

Che è la stessa soluzione ottenuta per la stima a massima verosimiglianza. Queste metodologie offrono una stima dei parametri di una funzione tramite la minimizzazione dei residui.
La soluzione è quella che minimizza lo scarto quadratico medio dei residui, ovvero è a minima varianza.

\end{document}
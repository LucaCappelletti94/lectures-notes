\providecommand{\main}{../../../}
\documentclass[\main/main.tex]{subfiles}
\begin{document}
\subsection{Che cosa è la stima della massima verosimiglianza?}
Si tratta di un metodo che realizza una stima dei parametri di una distribuzione partendo da un vettore di misurazioni $\bmy$ che corrisponde a delle realizzazioni di una variabile che si suppone appartenga alla distribuzione data. Per esempio nel caso di una distribuzione gaussiana si procede come segue:

\[
  L\rnd{\left. \bmy\,\right\rvert\, \mu, \sigma} = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\crl{-\frac{1}{2}\rnd{\frac{y_i-\mu}{\sigma}}^2}
\]
A questo punto di procede calcolando il logaritmo negativo di $L$, $-ln{L}$ e si calcolano le derivate parziali in funzione dei parametri. Queste vengono quindi poste pari a 0 per massimizzarle e si risolve in funzione del parametro di interesse.

\subsection{Dimostrare che la stima ai minimi quadrati è equivalente alla stima a massima verosimiglianza nel caso di errore Gaussiano sui dati. Cosa fornisce? Come?}
Considerando il caso in cui vogliamo stimare una retta $y = mx+q$, stimando il coefficiente angolare $m$ ed il parametro $q$, con $n$ misurazioni $\bmx$ e $\bmy$ e considerando l'errore di misura gaussiano, procediamo con la s\textbf{tima a massima verosimiglianza}.

Costruisco la funzione di verosimiglianza:

\[
  L\rnd{\left. \bmy, \bmx \, \right\rvert\, m, q} = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\crl{-\frac{1}{2}\rnd{\frac{y_i-mx_i - q}{\sigma}}^2}
\]
Calcolo le derivate parziali del logaritmo negativo della verosimiglianza:

Per il coefficiente angolare $m$:

\[
  \frac{\partial (-\ln{L})}{\partial m} = 0 \Rightarrow \frac{1}{2\sigma^2} \sum^n_{i=1} \rnd{y_i - mx_i - q}\rnd{-2x_i} = 0 \Rightarrow  m\sum^n_{i=1} x^2_i + q\sum^n_{i=1}x_i = \sum^n_{i=1}x_iy_i
\]
Per il parametro $q$:

\[
  \frac{\partial (-\ln{L})}{\partial q} = 0 \Rightarrow \sum^n_{i=1}\rnd{y_i - mx_i -q}x_i = 0 \Rightarrow  m\sum^n_{i=1}x_i + q\cdot n = \sum^n_{i=1}y_i
\]
In forma matriciale le equazioni ottenute sono le seguenti:

\[
  \begin{bmatrix}
    \sum^n_{i=1}x^2_i & \sum^n_{i=1}x_i \\
    \sum^n_{i=1}x_i   & n
  \end{bmatrix}
  \begin{bmatrix}
    m \\ b
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sum^n_{i=1}x_iy_i \\ \sum^n_{i=1}y_i
  \end{bmatrix}
\]

Procediamo ora con la \textbf{stima ai minimi quadrati}. Lo stesso problema nei minimi quadrati è impostato come segue:

\[
  m\bmx + b\bm{1} = \bmy
\]
SI tratta quindi di risolvere l'equazione:

\[
  \begin{bmatrix}
    \bmx^T \\ \bm{1}^T
  \end{bmatrix} \begin{bmatrix}
    \bmx & \bm{1}
  \end{bmatrix} \begin{bmatrix}
    m \\ b
  \end{bmatrix} = \begin{bmatrix}
    \bmx^T \\ \bm{1}^T
  \end{bmatrix} \bmy
  \Rightarrow
  \begin{bmatrix}
    \bmx\bmxt    & \bmxt\bm{1}    \\
    \bm{1}^T\bmx & \bm{1}^T\bm{1}
  \end{bmatrix}
  \begin{bmatrix}
    m \\ b
  \end{bmatrix}
  =
  \begin{bmatrix}
    \bmx^T\bmy \\ \bm{1}^T\bmy
  \end{bmatrix}
  \Rightarrow
  \begin{bmatrix}
    \sum^n_{i=1}x^2_i & \sum^n_{i=1}x_i \\
    \sum^n_{i=1}x_i   & n
  \end{bmatrix}
  \begin{bmatrix}
    m \\ b
  \end{bmatrix}
  =
  \begin{bmatrix}
    \sum^n_{i=1}x_iy_i \\ \sum^n_{i=1}y_i
  \end{bmatrix}
\]
I risultati ottenuti dai due metodi coincidono.

Entrambi i metodi offrono una stima dei parametri, ma coincidono solo quando il rumore segue una distribuzione normale a media nulla.

La stima alla massima verosimiglianza massimizza la probabilità condizionata che i singoli punti assumano tali valori per i parametri, quella ai minimi quadrati minimizza i quadrati dei residui: $\min_x (\matr{A}\bmx-\bmb)^2$.

\end{document}
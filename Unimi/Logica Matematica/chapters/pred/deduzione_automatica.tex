% TEX root = ../../main.tex
\chapter{Risoluzione Automatica}
Lo scopo del nostro corso è analizzare come sia possibile capire se una certa 
teoria $\Gamma \stackrel ? \models A$ in modo automatico. 
Se $\Gamma = \{\gamma_1, \cdots, \gamma_n\}$, $\Gamma \models A$ se e solo se 
$\gamma_1 \land \gamma_2 \land \cdots \land \gamma_n \land \neg A$ è insoddisfacibile. 
Quindi, informalmente, il nostro scopo è, dato $A$ un $\mathscr{L}$-enunciato, stabilire se $A$ 
è insoddisfacibile. 


\section{Forme Normali}
Cercheremo di seguire il più possibile ciò che è stato fatto anche per la Logica 
Proposizionale, ossia riportare il tutto in CNF per poi applicare qualcosa 
di simile al calcolo refutazionale - tuttavia, nella Logica dei Predicati, 
questo passaggio è leggermente più complicato. 

Prima di esaminare questa procedura, diamo delle definizioni utili. 
\begin{defi}[Equivalenza Logica]
Siano $A$ e  $B$ $\mathscr{L}$-enunciati. $A$ è logicamente equivalente a $B$ sse $A$ e $B$ hanno gli stessi modelli per ogni $\mathscr{L}$-struttura, cioè:
$$
A \equiv B \iff \forall \mathscr{A} ~~ (\mathscr{A} \models A \iff \mathscr{A} \models B)
$$
Equivalentemente si può definire tramite la seguente verità logica: 
$$
\models (A \rightarrow B) \land (B \rightarrow A) 
$$
\end{defi}

\begin{defi}[Equisoddisfacibilità]
Siano $A$ e $B$ due $\mathscr{L}$-enunciati. $A\ equisodd.\ B$ sse  $A$ ha un modello sse $B$ ha un modello; anche se sono modelli diversi. 
\end{defi}

\begin{defi}[Forma Normale Prenessa (PNF)]
Un $\mathscr{L}$-enunciato è in PNF sse è nella forma 
\[
\overbrace{Q_1 x_1, \ldots, Q_n x_n}^{\text{prefisso}} \overbrace{M}^{\text{matrice}}
\]
dove $Q_i \in \{ \forall, \exists\}$ e $M$ una F.b.F. priva di quantificatori. 

\end{defi}
Ogni $\mathscr{L}$-enunciato è logicamente equivalente ad un $\mathscr{L}$-enunciato in PNF, 
ossia 

$$
A \equiv A^P, \text{ con } A^P \in PNF
$$

$A^P$ si costruisce efficientemente da $A$ utilizzando la ripetuta riscrittura tramite 
\textbf{equivalenze logiche notevoli}, ossia partendo da un certo 
$A_0$, si arriva in un numero limitato di passi ad $A_N$ che è in $PNF$: 
$$
A := A_0 \leadsto A_1 \leadsto A_2 \ldots \leadsto A_n = A^P \in PNF
$$

\subsection{Equivalenze Notevoli}
Sia $P$ una F.b.F. e sia $z$ una variabile che \textbf{non occorre} (né libera 
né vincolata) in $P$. 

\subsubsection{Rinomina}
\label{rinomina}
\[
\begin{cases}
  \exists x P \equiv \exists z P[z/x] \\
  \forall x P \equiv \forall z P[z/x]
\end{cases}
\]
\subsubsection{De Morgan}
\[
\begin{cases}
  \neg \forall x P \equiv \exists x \neg P \\
  \neg \exists x P \equiv \forall x \neg P \\
  \forall x P \equiv \neg \exists x \neg P \\
  \exists x P \equiv \neg \forall x \neg P
\end{cases}
\]
\begin{proof}{di (1)}
  \begin{align*}
    & \mathscr{A} \models \neg \forall x P \\
    \iff & \mathscr{A} \nvDash \forall x P \\
    \iff & \text{non è vero che } \forall a \in A ~~ \mathscr{A} \models P[\bar{a}/x] \\
    \iff & \text{non per tutti gli } a \in A ~~ \mathscr{A} \models P[\bar{a}/x] \\
    \iff & \text{ per almeno un } a \in A ~~ \mathscr{A} \nvDash P[\bar{a}/x] \\
    \iff & \text{ esiste almeno un } a \in A ~~ \mathscr{A} \models \neg P[\bar{a}/x] \\
    \iff & \mathscr{A} \models \exists x \neg P 
  \end{align*}
\end{proof}

\subsubsection{Quantificazione Ridondante}
$$
\begin{cases}
  \forall x \forall y ~~ P  \equiv \forall y \forall x ~~ P \\
  \exists x \exists y ~~ P  \equiv \exists y \exists x ~~ P \\
  \forall x ~~ P \equiv P  & \text{se } x \notin FV(P) \\
  \exists x ~~ P \equiv P  & \text{se } x \notin FV(P)
\end{cases}
$$

\`E importante notare che invece non vale, in genere, $\forall x \exists y ~~ P \equiv \exists y \forall x ~~ P$.  


\paragraph{Quantificatore rispetto a $\land$ e $\lor$}
$$
\begin{cases}
        \forall x ~~ (P_1 \land P_2) \equiv \forall x ~~ P_1 \land \forall x ~~ P_2 \\
        \exists x ~~ (P_1 \lor P_2) \equiv \exists x ~~ P_1 \lor \exists x ~~ P_2 \\
        \forall x ~~ (P_1 \lor P_2) \equiv (\forall x ~~ P_1) \lor P_2 & \text{se } x \notin FV(P_2) \\
        \exists x ~~ (P_1 \land P_2) \equiv (\exists x ~~ P_1) \land P_2 & \text{se } x \notin FV(P_2)
\end{cases}
$$
Si noti che, per esempio, non valgono:
\begin{align*}
\forall x ~~ (P_1 \lor P_2) \neq \forall x ~~ P_1 \lor \forall x ~~ P_2 &&
\exists x ~~ (P_1 \land P_2) \neq \exists x ~~ P_1 \land \exists x ~~ P_2
\end{align*}

Siano, infatti, $P$ l'insieme dei numeri pari, mentre $D$ l'insieme dei numeri dispari. Allora:
\begin{align*}
\mathbb{N} \models \forall x (P(x) \lor D(x)) &&
\mathbb{N} \nvDash \forall x P(x) \lor \forall x D(x)
\end{align*}

Analogamente:
\begin{align*}
\mathbb{N } \nvDash \exists x (P(x) \land D(x)) &&
\mathbb{N} \models \exists x P(x) \land \exists x D(x)
\end{align*}

\subsubsection{Trasformazione in FNP}
Il modo corretto di mettere una formula precedente in Forma Normale Prenessa è usando la rinomina:
$$
\exists x P(x) \land \exists x D(x) \equiv \exists x \exists y (P(x) \land D(y))
$$
\begin{proof}
\begin{align*}
  \exists x P(x) \land \exists x D(x) & \equiv \exists x (P(x) \land \exists x D(x)) & \text{perché } x \notin FV(\exists x D(x)) \\
  & \equiv \exists x (P(x) \land \exists y D(y)) & \text{Rinomina~(\ref{rinomina})} \\
  & \equiv \exists x \exists y (P(x) \land D(y)) & \text{perché } y \notin FV(\exists x D(x)) \\
\end{align*}
\end{proof}

\subsubsection{Altre Equivalenze Notevoli}
Dati due quantificatori $Q_1, Q_2 \in \{\forall, \exists\}$:
\[
\begin{cases}
  Q_1 x P_1 \lor Q_2 x P_2 \equiv Q_1 x Q_2 z (P_1 \lor P_2[z/x]) \\
  Q_1 x P_1 \land Q_2 x P_2 \equiv Q_1 x Q_2 z (P_1 \land P_2[z/x])
\end{cases}
\]
con $z \notin FV(P_1) \cup FV(P_2)$, o in generale una nuova variabile. \\
Inoltre, se $x \notin FV(Q)$ (o con rinomina): 
$$
\begin{cases}
  \forall x P \rightarrow Q \equiv \exists x (P \rightarrow Q) \\
  \exists x P \rightarrow Q \equiv \forall x (P \rightarrow Q) \\
  Q \rightarrow \exists x P  \equiv \forall  x (Q \rightarrow P) \\
  Q \rightarrow \forall x P  \equiv \exists x (Q \rightarrow P)
\end{cases}
$$

\noindent
Dato un $\mathscr{L}$-enunciato $A$, c'è una sequenza
$$
A := A_0 \leadsto A_1 \leadsto A_2 \ldots \leadsto A_n = A^P \in PNF
$$
e ogni passaggio  $A_i \leadsto A_{i+1}$ è ottenuto applicando una delle equivalenze 
notevoli elencate, dunque $A \equiv A^P$. 
\begin{oss}
        Il numero di equivalenze notevoli da utilizzare per la trasformazione è 
        $$
        n \leq p(||A||)
        $$
        per un qualche polinomio $p$. 
\end{oss}

\subsection{Forma Normale di Skolem}
Si vuole arrivare nuovamente al calcolo refutazionale basato su \textit{risoluzione}, ma per poter dimostrare che una certa formula è insoddisfacibile non basta che sia in PNF, ma deve essere anche in Forma Normale di Skolem. 

\begin{defi}[Forma Normale di Skolem]
Un $\mathscr{L}$-enunciato $A \in PNF$ si dice in Forma Normale di Skolem sse non contiene occorrenze del quantificatore esitenziale.
\begin{align*}
\text{sia } & A := Q_1 x_1 Q_2 x_2 \cdots Q_n x_n M \\
\text{allora } &  A \in SNF \iff Q_i = \forall i \in n
\end{align*}
\end{defi}

Vi sono altre traduzioni di calcolo, che invece mirano alla prova diretta 
(come i calcoli alla Hilbert per la Logica Proposizionale) in cui si vuole 
eliminare il quantificatore universale. 

\subsubsection{Skolemizzazione}
Vi sarà dunque un procedimento, chiamato \textbf{skolemizzazione}, 
che porta un generico $\mathscr{L}$-enunciato $A$ in 
Forma Normale Prenessa ad un enunciato \textbf{equisoddisfacibile} in Forma Normale 
di Skolem.

\paragraph{Esempio di Skolemizzazione}
Riprendiamo le due formulazioni date della teoria dei gruppi: 
$$
\Gamma_{G_1} := 
  \begin{cases}
    \forall x \forall y \forall z  ~~ ((x * y) * z) = (x * (y * z))  & \text{ associatività}\\
    \forall x (x * e) = x \land (e * x) = x  & \text{ elemento neutro}\\
    \forall x (x * x^{-1}) = e \land (x^{-1} * x = e) & \text{ invertibilità}
  \end{cases}
$$

$$
        \Gamma_{G_2} := 
                \begin{cases}
                        \forall x \forall y \forall z  ~~ ((x * y) * z) = (x * (y * z)) & \text{ associatività}\\
                        \forall x (x * e) = x \land (e * x) = x & \text{ elemento neutro } \\
                        \forall x \exists y (x * y) = e \land (y * x) = e & \text{ esistenza inverso} \\
                \end{cases}
$$

La seconda formulazione non è equivalente alla prima, tuttavia sono perlomeno 
equisoddisfacibili. Si può notare che nella seconda formulazione vi è un assioma 
con un quantificatore esistenziale. Si può pensare che la prima 
formulazione sia la Forma Skolemizzata della seconda formulazione. 
 
Nella seconda formulazione, il terzo assioma è descritto come 
$$
\forall x \exists y (x * y) = e \land (y * x) = e
$$
Questo sarà vero in una struttura (che sarà un gruppo) $\mathscr{A} = (U,I)$ se e 
solo se per ogni $a \in U$ esiste $b \in U$ tale che $\bar{a} * \bar{b} = e$ 
e $\bar{b} * \bar{a} = e$. L'idea sottesa per portare questo assioma 
nella sua Forma Skolemizzata è che si può associare all'esistenza di $b$ una 
certa funzione di $a$, in questo caso \textit{l'inverso} di $a$. In altre 
parole, ad ogni $a$ si associa un elemento $f(a)$ tale che l'assioma sia verificato. 
Nell'esempio concreto, si arriva dunque a definire una nuova $\mathscr{L}$-struttura $\mathscr{A}'$
in modo tale che 
$$
\mathscr{A}' \models \forall x (x*x^{-1} = e \land x^{-1} * x = e)
$$
La funzione inverso è esattamente il ``gioco'' sotteso alla Skolemizzazione: 
ora, l'assioma legge che $\mathscr{A}'$ è un modello per l'assioma 
se e solo se per ogni $a \in U$ si ha che $\bar{a} * \bar{a}^{-1} = e$ 
e $\bar{a}^{-1} * \bar{a} = e$, postulando in qualche senso che  la funzione
$f(a) = b$ esiste ed è chiamata inverso. 

\`E chiaro che questo processo non può preservare l'equivalenza logica per 
due motivi: il primo, superficiale, è che i due $\mathscr{L}$-enunciati 
sono scritti in due linguaggi diversi. Il secondo, più profondo, è che 
quando si interpreta il nuovo simbolo $f$, c'è \textit{un} modo per interpretarlo 
bene, ma esistono anche tutte le altre interpretazioni che possono 
far fallire la formula; l'unica cosa che si sa è che c'è \textit{almeno una}
interpretazione che possa far funzionare le cose. 

\paragraph{Processo di Skolemizzazione}
Questo discorso è assolutamente generale ed è il cuore della Skolemizzazione: 
$$
\mathscr{A} \models A =  \forall x_1\cdots\forall x_n \exists y B
$$
Senza perdità di generalità, si assume che in $A$ non occorrano quantificazioni ridondanti, cioè in $B$ non occorrono cose come $\forall x$, $\exists x$, in altre parole ogni occorrenza di $x_1, \cdots, x_n, y \in B$ è libera. In secondo luogo, in $B$ possono occorrere altri quantificatori, pertanto $\exists y$ è solo l'occorrenza ``più esterna''. 

Applicando la definizione di semantica di Tarski si legge che quanto è 
scritto è vero se e solo se per ogni $(a_1, \cdots, a_n) \in U^n$ esiste 
un $b \in U$ tale che 
$$
\mathscr{A} \models A = B[\bar{a}_1/x_1]\cdots[\bar{a}_n/x_n][\bar{b}/y]
$$
il concetto di esistenza (di $b$ in questo caso) viene riletto come un modo 
per associare un immagine ad un argomento di una qualche funzione: esisterà quindi
$$
f(a_1, \cdots, a_n) = b
$$
Ciò che la Skolemizzazione ``fa'' è introdurre un simbolo di funzione $n$-ario $\bar f$ nel linguaggio interpretandolo come la funzione $f$; dunque, la $\mathscr{L}$-struttura iniziale viene modificata in una $\mathscr{L}$-struttura estesa che deve interpretare anche $\bar f$:
$$
\mathscr{A}' \models A^S = \forall x_1 \cdots \forall x_n B[f(x_1, \cdots, x_n)/y]
$$
creando quindi $A^S$, la Forma Skolemizzata di $A$. \\
Allo stesso modo si passa da un linguaggio $\mathscr{L}$ ad uno $\mathscr{L}^S$:
\begin{align*}
  \mathscr{L} \leadsto \mathscr{L}^S = (\mathscr{P}, \mathscr{F } \cup \{f\}, \alpha, \beta^S) &&
  \text{con }
  \begin{cases}
    \beta^S(f) = n \\
    \beta^S(x) = \beta(x) & \text{altrimenti}
  \end{cases}
\end{align*} 
E da una $\mathscr{L}$-struttura ad una $\mathscr{L}^S$-struttura:
\begin{align*}
  \mathscr{A} = (U,I) \leadsto (\mathscr{A}, h) = (U, I_h) &&
  \text{con }
  \begin{cases}
    I_h(f) = h \\
    I_h(x) = I(x) & \text{altrimenti}
  \end{cases}
\end{align*}
Dove $h$ è una funzione $n$-aria $h: U^n \rightarrow U$

\paragraph{Equisoddisfacibilità}
Definiamo $A$ e la sua forma Skolemizzata $A^S$
\begin{align*}
A := \exists x P(x) && 
A^S := P[c/x] = P(c)
\end{align*}
Sia $\mathscr{A} = (U, I)$ la $\mathscr{L}$-struttura con: 
\begin{align*}
U    = \{e_1,e_2\} &&
I(P) = \{e_2\} 
\end{align*}
Allora $(\mathscr{A}, e_2) \models A$ e $(\mathscr{A}, e_1) \models A$, infatti: 
\begin{align*}
  (\mathscr{A}, e_2) \models A &\iff (\mathscr{A}, e_2) \models \exists x P(x)\\
    &\iff \text{ esiste } a \in U \text{ tale che } \mathscr{A} \models P(x)[\bar{a}/x] \\
    &\iff \text{ esiste } a \in U \text{ tale che } \mathscr{A} \models P(a) \\
    &\iff \text{esiste: } a = e_2 ~~~ \mathscr{A} \models P(e_2) & \text{perché }I_{e_2}(\bar e_2) \in I_{e_2}(P) \\
    & \iff \top & \text{perchè } e_2 \in \{e_2\}
\end{align*}
Parimenti, si ha anche che 
\begin{align*}
  (\mathscr{A}, e_1) \models A &\iff (\mathscr{A}, e_1) \models \exists x P(x)\\
      &\iff \text{ esiste } a \in U \text{ tale che } \mathscr{A} \models P(x)[\bar{a}/x] \\
    &\iff \text{ esiste } a \in U \text{ tale che } \mathscr{A} \models P(a) \\
    &\iff \text{esiste: } a = e_2 ~~~ \mathscr{A} \models P(e_2) & \text{perché }I_{e_1}(\bar e_2) \in I_{e_1}(P) \\
    & \iff \top & \text{perchè } e_2 \in \{e_2\}
\end{align*}
L'interpretazione della funzione di Skolem (in questo caso la costante $c$) è irrilevante per la valutazione di $A$!

Al contrario, se si interpreta erroneamente $c$, $A^S$ non è soddisfatta: 
\begin{align*}
  (\mathscr{A}, e_1) \nvDash A^S &\iff (\mathscr{A}, e_1) \nvDash P[c/x] = P(c) \\
    &\iff I_{e_1}(c) \not\in I_{e_1}(P) \\
    &\iff e_1 \not\in \{e_2\}
\end{align*}
Ossia, abbiamo dimostrato che:
\begin{align*}
A \not\equiv A^S && \text{e} && \nvDash A \rightarrow A^S
\end{align*}
e non possono, pertanto, essere logicamente equivalenti. Tuttavia sono equisoddisfacibili: 
$$
\models A^S \rightarrow A
$$
\begin{proof}[Dimostrazione di $\models A^S \rightarrow A$]
Siano $A$ una formula in PNF e $A^S$ la sua Skolemizzata:
\begin{align*}
A := \forall x_1 \cdots \forall x_n \exists y B &&
A^S := \forall x_1 \cdots \forall x_n B[f(x_1, \cdots, x_n)/y]
\end{align*}
E sia $(\mathscr{A}, h)$ la $\mathscr{L}^S$-struttura che soddisfa $A^S$, con $\mathscr{A} = (U,I)$. Allora: 
\begin{align*}
& (\mathscr{A}, h) \models A^S \\
\iff & (\mathscr{A}, h) \models B[f(x_1, \cdots, x_n)/y][\bar a_1 / x_1] \cdots [\bar a_n / x_n] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n\\
\iff & (\mathscr{A}, h) \models B[\bar a_1 / x_1]\cdots[\bar a_n / x_n][f(\bar a_1, \cdots , \bar a_n) / y] & \text{ per ogni } (a_1 \cdots, a_n) \in U^n \\
& \text{visto che in } B \text{ non vi sono occorrenze libere di } x_1, \cdots, x_n \\
\iff & (\mathscr{A}, h) \models B[\bar a_1 / x_1]\cdots[\bar a_n / x_n][\bar b / y] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
& \text{posto } b := h(a_1, \cdots, a_n) \text{, ricordandosi che } I_h(f) = h \\
\iff & \text{esiste un } b \in U \text{ t.c. } (\mathscr{A}, h) \models B[\bar a_1 / x_1]\cdots[\bar a_n / x_n][\bar b / y] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
\iff & (\mathscr{A}, h) \models \forall x_1 \cdots \forall x_n \exists y B \\
\iff & (\mathscr{A}, h) \models A\\
\Longrightarrow & \models A^S \rightarrow A
\end{align*}
\end{proof}

Questo non basta a dimostrare che siano equisoddisfacibili. Mostreremo ora 
che $A$ e $A^S$ sono equisoddisfacibili: si deve mostrare che 
se $(\mathscr{A}, h) \models A$ allora esiste $g:U^n\rightarrow U$ tale che 
$(\mathscr{A}, g) \models A^S$. 
\begin{proof}
\begin{align*}
  & (\mathscr{A}, h) \models A \\
  \iff & \text{ esiste } b \in U \text{ t.c. } (\mathscr{A}, h) \models B[\bar a_1 / x_1] \cdots [\bar a_n / x_n][\bar b / y] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
  \iff & \text{ esiste } b \in U \text{ t.c. } \mathscr{A} \models B[\bar a_1 / x_1] \cdots [\bar a_n / x_n][\bar b / y] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
  & \text{perché non vi sono occorrenze} \\
  & \text{del simbolo ``} f \text{'' da interpretare in ``} h \text{''}
\end{align*}
Si definisce ora la funzione $g: U^n \rightarrow U$:
$$
g(a_1, \cdots, a_n) := b ~~~ \text{ per ogni \textit n-pla } (a_1, \cdots, a_n) \in U^n
$$
Ci si chiede ora quando vale 
\begin{align*}
& (\mathscr{A}, g) \models A^S \\
\iff & (\mathscr{A}, g) \models B[f(x_1, \cdots, x_n)/y][\bar a_1/x_1]\cdots[\bar a_n/x_n] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
  \iff & (\mathscr{A}, g) \models B[\bar a_1/x_1]\cdots[\bar a_n/x_n][f(\bar a_1, \cdots, \bar a_n)/y] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
  \iff & (\mathscr{A}, g) \models B[\bar a_1/x_1]\cdots[\bar a_n/x_n][\bar b/y] & \text{ per ogni } (a_1, \cdots, a_n) \in U^n \\
  \iff & (\mathscr{A}, g) \models A & \text{per passaggio precedente }
\end{align*}
pertanto
$$
\text{ se } (\mathscr{A}, h) \models A \Longrightarrow \text{ esiste } g: U^n \rightarrow U \text{ t.c. } (\mathscr{A}, g) \models A^S 
$$
Con $h$ e $g$ interpretazioni di $f$ della prima—non usata—e della seconda $\mathscr{L}$-Struttura, rispettivamente.
\end{proof}

\subsection{Forma Normale Congiuntiva}
Sia $A$ un $\mathscr{L}$-enunciato. Siamo passati da $A$ ad $A^P \in PNF$ mantenendo l'equivalenza logica per poi passare a $A^S \in SNF$ mantenendo, in genere, solo l'equisoddisfacibilità. Ciò che manca per ricalcare il lavoro fatto riguardo la Logica Proposizionale, è portare il tutto in CNF: 
$$
A \xrightarrow{\equiv} A^P \xrightarrow{\text{equisod.}}(A^P)^S \leadsto ((A^P)^S)^C
$$
Quindi, $A^{p^S}$ è nella forma $\forall x_1 \cdots \forall x_n M$, dove in $M$ non occorrono quantificatori. Inoltre, $A^{p^S}$ è un $\mathscr{L}$-enunciato, pertanto $FV(M) \subseteq \{x_1, \cdots, x_n\}$. \\ 
Consideriamo ora ogni occorrenza di formula atomica $P(t_1, \cdots, t_n)$ in $M$ come una lettera proposizionale; $M$ può dunque essere pensata come una formula della Logica Proposizionale e il processo di trasformazione verso la CNF si ottiene applicando uno degli algoritmi visti, preservando equivalenza logica—con la distributività—o solo l'equisoddisfacibilità. Si ottiene, quindi: 
$$
M \xrightarrow[\text{equisod.}]{\equiv} M^C
$$

Concludendo, quindi che il processo di ``traduzione'' è 
$$
A \xrightarrow{\equiv} A^P \xrightarrow{\text{equisod.}}(A^P)^S \xrightarrow[\text{equisod.}]{\equiv} ((A^P)^S)^C
$$

\section{Risoluzione Automatica}
\subsection{Preprocessamento}
Siamo partiti dal chiederci se 
$$
\Gamma \stackrel ? \models A
\xrightarrow[(0)]{} \Gamma, \neg A \text{ insod.?}
\xrightarrow[(1)] {\text{ se } \Gamma \text{ finito}} \gamma_1 \land \cdots \land \gamma_n \land \neg A \text{ insod.?}
\xrightarrow[(2)]{} (\gamma_1 \land \cdots \land \gamma_n \land \neg A)^{P^{S^C}} \text{ insod.?}
$$
manca un'ultima trasformazione. \\
Infatti, sia $F \in PNF \cap SNF \cap CNF$  un enunciato in forma prenessa, Skolemizzata 
e congiuntiva:
\begin{align*}
& F = \forall x_1 \cdots \forall x_n C_1 \land C_2 \land \cdots \land C_u \\
& \text{dove ogni clausola } C_i = (\ell_1 \lor \ell_2 \lor \cdots \lor \ell_v) \\
& \text{dove ogni letterale } \ell_{i_j} =
  \begin{cases}
    P(t_1, \cdots, t_w) \\
    \neg P(t_1, \cdots, t_w)
  \end{cases}
  \text{ per un qualche } P(t_1, \cdots, t_w) \text{ formula atomica}
\end{align*}
Operiamo, quindi, i seguenti passaggi di notazione: 
\begin{itemize}
  \item Omettiamo i quantificatori universali. Possiamo farlo per due ragioni: 
    \begin{enumerate}
      \item Scriverli esplicitamente non aggiunge nessuna informazione: sappiamo che tutte le variabili che occorrono sono quantificate universalmente.
      \item $\mathscr{A} \models F$ se e solo se $\mathscr{A} \models \forall[F]$, con $F$ F.b.F.
    \end{enumerate}
  \item Notazione a sequenti. \\
    Si riscrive la formula $F = C_1 \land \cdots \land C_2$ come insieme delle sue clausole:
    $$
    S_F = \{C_1, \cdots, C_n\}
    $$
    Questa riscrittura ha vari vantaggi: 
    \begin{itemize}
      \item Ogni formula $\gamma_i$ in $\Gamma \cup \{\neg A\}$ si può mettere in PNF, SNF e CNF indipendentemente, evitando di creare una formula gigante al passaggio $(1)$ descritto sopra
      \item Se $\Gamma$ è infinito $S_{\Gamma}$ sarà un insieme infinito di clausole finite
    \end{itemize}
    Inoltre, si riscrive anche ogni clausola $C_i = \ell_{i_1} \lor \cdots \lor \ell_{i_v}$:
    \begin{align*}
    C_i = & \underbrace{\{\ell_{i_j} \text{ che occorrono negati, senza } \neg\}}_{= N_i} \cup \underbrace{\{\ell_{i_j} \text{ che occorrono positivi}\}}_{= P_i} \\
    C_i = & N_i \implies P_i
    \end{align*}
    Quest'ultima è una notazione \textit{a sequenti} della clausola $C_i$. 
\end{itemize}

\subsubsection{Esempio}
Ci chiediamo se sia vero che
$$
\forall x \exists y (R(x,y) \rightarrow Q(f(x))) \stackrel ? \models \forall x \exists y (R(x,y) \rightarrow Q(f(y)))
$$
Il primo passo è riportare il tutto in una domanda di insoddisfacibilità: 
$$
\forall x \exists y (R(x,y) \rightarrow Q(f(x))), \neg (\forall x \exists y (R(x,y) \rightarrow Q(f(y)))) \text{ insod.?}
$$
Si procede, ora, mettendo in PNF, SNF e CNF le due formule.
La prima, 
$$
\forall x \exists y (R(x,y) \rightarrow Q(f(x)))
$$
è già in PNF. Si procede sostituendo a $y$ un simbolo di funzione, creando la 
funzione di Skolem utilizzando un nuovo simbolo: 
$$
\forall x (R(x, s(x)) \rightarrow Q(f(x)))
$$
e infine in CNF (o a sequenti): 
$$
\forall x (\neg R(x, s(x)) \lor Q(f(x)))
$$
concludendo con 
$$
C_1 := \{\neg R(x,s(x)), Q(f(x))\}
$$
e scritto in forma a sequenti 
$$
S_1 := \overbrace{R(x, s(x))}^{N_1} \implies \overbrace{Q(f(x))}^{P_1}
$$

La seconda formula 
$$
\neg (\forall x \exists y (R(x,y) \rightarrow Q(f(y)))) = \exists x \forall y \neg (R(x,y) \rightarrow Q(f(y)))
$$
è già in PNF, e per portarla in SNF si sostituisce a $x$ un simbolo di funzione
(costante), utilizzando un nuovo simbolo:
$$
\forall y \neg (R(c, y) \rightarrow Q(f(y)))
$$
e si porta in CNF: 
$$
\neg (R(c,y) \rightarrow Q(f(y))) \equiv
\neg (\neg R(c,y) \lor Q(f(y))) \equiv
R(c,y) \land \neg Q(f(y))
$$
ottenendo  due clausole: 
$$
C_2 = \{R(c,y)\}
$$
$$
C_3 = \{\neg Q(f(y))\}
$$
e due sequenti: 
$$
S_2 : \implies \overbrace{R(c,y)}^{P_2}
$$
$$
S_3 : \overbrace{Q(f(y))}^{N_3} \implies
$$
E si riscrive, infine, 
\begin{align}
\label{enuciato-in-sequenti}
  \{ R(x,s(x)) \implies Q(f(x)), \implies R(c,y), Q(f(y)) \implies\} \text{ insod.?}
\end{align}
Vedremo ora come rispondere a questa domanda. \\
Spoiler: è un problema semidecidibile, anche quando c'è un numero finito di clausole in notazione a sequenti. Questo perché potrebbero comunque codificare infinite molte informazioni! 

\subsection{Teoria di Herbrand}
Sia $S$ un insieme di clausole nella Logica dei Predicati; quindi, si immagina $S$ 
ottenuto mettendo in PNF, SNF, CNF uno statement del tipo $\Gamma \models A$, 
di cui vogliamo stabilire la soddisfacibilità. Questo sarà eseguito riducendo 
$S$ insoddisfacibile a un problema di insoddisfacibilità nella Logica Proposizionale.

Il primo passo è la seguente definizione: 
\begin{defi}[Astrazione Proposizionale]
  L'\textbf{astrazione proposizionale} è una funzione che associa iniettivamente ad ogni \textit{formula atomica} ground (quindi nella forma $P(t_1, \cdots, t_n)$ dove ogni $t_i$ è un termine ground, ossia non appaiono variabili) una lettera proposizionale, denotata con $p_{P(t_1, \cdots, t_n)}$. 
\end{defi}

Per esempio, se una clausola $C$ espressa in sequenti è 
$$
C: A_1, \cdots, A_u \implies B_1, \cdots, B_u
$$
la sua astrazione proposizionale è la clausola ottenuta astraendo ogni lettera: 
$$
C: p_{A_1}, \cdots, p_{A_u} \implies p_{B_1}, \cdots, p_{B_u}
$$
La \textbf{Teoria di Herbrand} ha suo culmine in quello che è ormai noto 
come \textbf{Teorema di Herbrand}, anche se nella letteratura ci sono delle 
perplessità riguardo il suo status di \textit{teorema}: 
\begin{teo}[di Herbrand]
        Un insieme di clausole $S$ della Logica dei Predicati è insoddisfacibile 
        se e solo se esiste un insieme finito di istanze ground di clausole 
        di $S$ la cui astrazione proposizionale è (proposizionalmente) insoddisfacibile.
\end{teo}

Per approfondire la Teoria di Herbrand, introduciamo delle nozioni. 

\begin{defi}[Istanziazione Ground]
Sia $S$ un insieme di clausole del Primo Ordine, e sia $G$ un insieme di termini ground. Allora, con la notazione $S/G$ indichiamo l'istanziazione ground di $S$ su $G$, vale a dire l'insieme di tutte le clausole (che risulteranno ground) ottenute da ciascuna clausola $C \in S$ sostituendo in $G$ ogni sua variabile $x$ con qualche $t \in G$, in modo simultaneo, in tutti i modi possibili.
\end{defi}
\paragraph{Esempio}
Sia 
\begin{align*}
        S &:= \{\{\implies P(x),D(x)\}, \{P(x), D(x) \implies\}\} \\
          &= \{\{P(x), D(x)\}, \{\neg P(x), \neg D(x)\}\} \\
          &= (P(x) \lor D(x)) \land (\neg P(x) \lor \neg D(x))
\end{align*}
Sia $G$ ad esempio 
$$
G = \{a, b, c\}
$$
Allora $S/G$ è 
\begin{align*}
S/G := &\{\{\implies P(a),D(a)\}, \{\implies P(b), D(b)\}, \{\implies P(c), D(c)\}, \\
       &\{P(a),D(a) \implies\},\{P(b),D(b) \implies\},\{P(c),D(c) \implies\}\}
\end{align*}
Sia ora 
$$
G' = \{a, f(b,c)\}
$$
allora $S/G'$ è 
\begin{align*}
        S/G := &\{\{\implies P(a),D(a)\}, \{\implies P(f(b,c)), D(f(b,c))\}, \\
               &\{P(a),D(a) \implies\},\{P(f(b,c)),D(f(b,c)) \implies\}\}
\end{align*}


\noindent 
Siamo interessati ad istanziare $S$ su un particolare insieme $G$, ossia quello 
che ci permette migliormente di studiare il nostro problema: questo 
insieme è chiamato \textit{Universo di Herbrand}:
\begin{defi}[Universo di Herbrand]
L'Universo di Herbrand $H_S$ di un insieme di clausole $S$ del Primo Ordine è definito come segue. \\
Sia $\mathscr{F}^0_S$ l'insieme di tutti i simboli di funzione—anche le costanti—che occorrono in $S$.
\begin{itemize}
  \item Se $\mathscr{F}^0_S$ non contiene costanti, allora si pone $\mathscr{F}_S = \mathscr{F}^0_S \cup \{\mathcal{z}\}$, dove $\mathcal{z}$ è una costante nuova.
  \item Se $\mathscr{F}^0_S$ contiene qualche costante, allora $\mathscr{F}_S := \mathscr{F}^0_S$.
\end{itemize}
$H_S$ è per definizione l'insieme di tutti i termini costruibili utilizzando i simboli di $\mathscr{F}_S$, che sono necessariamente ground. 
\end{defi}

\subsubsection{Esempi}
\paragraph{1}
Sia $S := \{\{U(x) \implies M(x)\}, \{\implies U(s)\}, \{M(s) \implies\}\}$, allora:
\begin{align*}
\mathscr{F}^0_S = \{s\} &&
\mathscr{F}_S = \{s\} &&
H_S = \{s\}
\end{align*}
\paragraph{2}
Sia $S := \{\{\implies P(x),D(x)\}, \{P(x), D(x) \implies\}\}$, allora: 
\begin{align*}
\mathscr{F}^0_S = \emptyset &&
\mathscr{F}_S = \{\mathcal{z}\} &&
H_S = \{\mathcal{z}\}
\end{align*}
\paragraph{3}
Sia $S := \{\{\implies P(f(x)),D(x)\}\}$, allora: 
\begin{align*}
\mathscr{F}^0_S = \{f(\cdot)\} &&
\mathscr{F}_S = \{f(\cdot),\mathcal{z}\} &&
H_S = \{\mathcal{z}, f(\mathcal{z}), f(f(\mathcal{z})), \cdots\}
\end{align*}

\noindent
Argomenteremo che $S$ è insoddisfacibile se e solo se $(S/H_S)^*$, ossia l'astrazione proposizionale dell'istanziazione di $S$ sul suo Universo di Herbrand $H_S$ è insoddisfacibile; quest'ultimo insieme è spesso infinito, in quanto—oltre ai simboli di funzione ``nativi'' del linguaggio su cui è costruito $S$—è costruito a seguito della Skolemizzazione dell'insieme $S$. 

I due teoremi ``di Herbrand'' enunciati seguono entrambi piuttosto facilmente 
dalla semantica di Herbrand, che è il vero cuore della questione. 

\subsection{Semantica di Herbrand}
Ricordiamo i vicoli che una $\mathscr{L}$-struttura di Tarski $\mathscr{A} = (U,I)$ deve rispettare:
  \begin{itemize}
    \item $U$ scelto arbitrariamente t.c. $U \neq \emptyset$
    \item $I(c) \in U$, scelto arbitrariamente
    \item $I(f)$ (se $\beta(f) = n$) è una funzione $U^n \rightarrow U$ scelto arbitrariamente
    \item $I(P) \subseteq U^n$ (con $\alpha(P) = n$), scelto arbitrariamente
  \end{itemize}
Nella semantica di Herbrand si considerano solo le $\mathscr{L}$\textbf{-strutture di Herbrand}. 

\begin{defi}[$\mathscr{L}$-struttura di Herbrand]
  Una $\mathscr{L}$-struttura di Herbrand è un tipo particolare di $\mathscr{L}$-struttura \textit{di Tarski} $\mathscr{H} = (H_S, H)$, con i seguenti vincoli aggiuntivi: 
  \begin{itemize}
    \item $H_S$ è l'Universo di Herbrand, fissato. \\
    Per costruzione ha almeno un termine ground: $H_S \neq \emptyset$
    \item $H(c) \in H_S$, t.c. $H(c) = c$
    \item $H(f)$ (se $\beta(f) = n$) è una funzione $H_S^n \rightarrow H_S$ t.c.
    $$
    (H(f))(t_1, \overbrace{\cdots}^{\in H_S}, t_n) := f(t_1, \cdots, t_n)
    $$
    \item $H(P) \subseteq H_S^n$ (con $\alpha(P) = n$), scelto arbitrariamente.
  \end{itemize}
\end{defi}
Si noti che in una $\mathscr{L}$-struttura generica si ha un universo d'interesse, formato da oggetti fortemente semantici (e.g., ``numeri'', ``orbite di pianeti'', ``oggetti geometrici'',...). \\
$H_S$, invece, è un universo di termini, ed è quindi puramente sintattico. Con la semantica di Herbrand, sintassi e semantica diventano una cosa sola! Infatti $c$ ed $f$, oggetti sintattici, vengono interpretati in loro stessi, ma nella loro forma semantica.

\begin{teon}[Teorema Fondamentale della Teoria di Herbrand]
\label{thm:fondamentale-herbrand}
  Sia $S$ un insieme di clausole del Primo Ordine, allora:
  $$
  \text{esiste } \mathscr{A} = (U,I) \text{ t.c. } \mathscr{A} \models S \iff
  \text{esiste } \mathscr{H}=(H_s, H) \text{ t.c. } \mathscr{H} \models S
  $$
\end{teon}
\noindent
$S$ è soddisfacibile, ossia per $S$ esiste un modello (di Tarski) $\mathscr{A} = (U,I)$ tale che $\mathscr{A} \models S$ se e solo se $S$ ha un modello di Herbrand, ossia se esiste una $\mathscr{L}$-struttura di Herbrand $\mathscr{H}=(H_s, H)$ tale che $\mathscr{H} \models S$.

Ora, ci chiediamo quale sia la relazione tra $(S/H_S)^*$ e $\mathscr{H} \models S$.
\begin{lemn}
\label{lem:herbrand-sodd}
  $$
  \mathscr{H} \models S \iff \mathcal{v}_{\mathscr{H}} \models (S/H_S)^*
  $$
\end{lemn}
\begin{proof} ($\implies$) \\
Sia $\mathcal{v}_{\mathscr{H}}: \text{Lettere Proposizionali} \rightarrow \{0,1\}$, un assegnamento tale che:
$$
\mathcal{v}_{\mathscr{H}}(p_{P(t_1, \cdots, t_n)}) :=
\begin{cases}
  1 & \text{ sse } \mathscr{H} \models P(t_1, \cdots, t_n) \\
  0 & \text{ sse } \mathscr{H} \nvDash P(t_1, \cdots, t_n)
\end{cases}
$$
($\Longleftarrow$) \\
Viceversa, data $\mathcal{v}: \text{ Lettere Proposizionali} \rightarrow \{0,1\}$, si definisce:
$$
\mathscr{H}_{\mathcal{v}}= (H_S, H_{\mathcal{v}}) :=
\begin{cases}
  (t_1, \cdots, t_n) \in H_{\mathcal{v}}(P) & \text{ sse } \mathcal{v}(p_{P(t_1, \cdots, t_n)}) = 1 \\
  (t_1, \cdots, t_n) \not\in H_{\mathcal{v}}(P) & \text{ sse } \mathcal{v}(p_{P(t_1, \cdots, t_n)}) = 0
\end{cases}
$$
\end{proof}

\subsubsection{Teorema Riassuntivo (Herbrand, compattezza, completezza, calcolo Refutazionale $R^*$, DPP e DPLL)}
Dato un insieme $S$ di clausole della Logica del Primo Ordine, è equivalente affermare:
\begin{enumerate}
  \item $S$ è insoddisfacibile 
  \item $S$ non ha modelli di Herbrand \\
  {[\textit{per Thm~\ref{thm:fondamentale-herbrand} Fondamentale della Teoria di Herbrand}]}
  \item $(S/H_S)^*$ è proposizionalmente insoddisfacibile \\
  {[\textit{per Lemma~}\ref{lem:herbrand-sodd}]}
  \item Esiste $S' \subseteq_{\omega} (S/H_S)^*$ tale che $S'$ è proposizionalmente insoddisfacibile \\
  {[\textit{per Thm~\ref{thm:compattezza-prop} di Compattezza Proposizionale}]}
  \item Esiste $H'\subseteq_{\omega} H_S$ tale che $(S/H')^*$ è finito e proposizionalmente insoddisfacibile \\
  {[\textit{per compattezza e teoria degli insiemi}]}
  \item Esiste $H' \subseteq_{\omega} H_S$ e $h \geq 0$ tale che $\qedsymbol \in \mathscr{R}^h((S/H)^*)$ \\
  {[\textit{per Thm~\ref{thm:completezza-refutazionale} di Completezza Refutazionale}]}
  \item Esiste $H' \subseteq_{\omega} H_S$ e una refutazione $DPP \vdash_R (S/H')^*$ o anche $DPLL \vdash_R (S/H')^*$ \\
  {[\textit{per Thm di Completezza Refutazionale di DPP (\ref{thm:completezza-refutazionale-dpp}) e DLPP (\ref{dpll})}]}
\end{enumerate}
La catena $1 \cdots 7$ fornisce un algoritmo per \textit{semidecidere} l'insoddisfacibilità di $S$. Questo perché non sappiamo come prendere $H' \subseteq_\omega H_S$, e bisogna tentare con $S'$ via via più grandi. Ci si ferma solo quando se ne trova un $(S/H')^*$ refutabile. 

\subsection{Metodi Refutazionali}
Ritorniamo, ora, a rispondere alla domanda \ref{enuciato-in-sequenti}, ovvero: 
$$
  S = \{ R(x,s(x)) \implies Q(f(x)), \implies R(c,y), Q(f(y)) \implies\} \text{ insod.?}
$$
Si procede alla risoluzione del problema dell'insoddisfacibilità, seguendo 
i passi definiti precedentemente: 
\begin{align*}
\mathscr{F}_S = \{c, s(\cdot), f(\cdot, \cdot)\} &&
H_S = \{c, s(c), f(c), f(s(c)), s(f(c)), s(s(c)), \cdots\, \overbrace{f(\cdots s( \cdots (c) \cdots))}^{\text{numero finito di } s \text{ e } f}, \cdots\}
\end{align*}
Pertanto, è impossibile costruire $(S/H_S)^*$ direttamente, in quanto è anch'esso sarebbe infinito. Si passa, quindi, a considerare sottoinsiemi finti di $H_S$, per esempio:
\paragraph{(1)} $H = \{c\}$: 
$$
(S/H)^* = \{ p_{R(c,s(c))} \implies p_{Q(f(c))}, \implies p_{R(c,c)}, p_{Q(f(c))} \implies \}
$$
Possiamo provare ad effettuare la risoluzione 
\begin{prooftree}
  \AxiomC{$p_{R(c,s(c))} \implies p_{Q(f(c))}$}
  \AxiomC{$p_{Q(f(c))} \implies $}
  \LeftLabel{\scriptsize$\mathbb{R} \text{ con } \ell = p_{Q(f(c))}$}
  \BinaryInfC{$p_{R(c,s(c))} \implies$}
\end{prooftree}
e rimaniamo con 
$$
(S/H)^* = \{p_{R(c,s(c))} \implies, \implies p_{R(c,c)}\}
$$
coi quali non si può fare niente, in quanto le astrazioni proposizionali sono diverse. \\
\textit{NOTA}: d'ora in poi evitiamo di applicare le astrazioni proposizionali e considereremo i predicati come dei termini ground

\paragraph{(2)} $H'=\{c, s(c)\}$:
\begin{align*}
  (S/H')^* = &\{ R(c, s(c)) \implies Q(f(c)), \implies R(c,c), Q(f(c)) \implies, \\
    &R(s(c),s(s(c))) \implies Q(f(s(c))), \implies R(c,s(c)), Q(f(s(c))) \implies\}
\end{align*}
nuovamente:
\begin{prooftree}
  \AxiomC{$R(c,s(c)) \implies Q(f(c))$}
  \AxiomC{$Q(f(c)) \implies $}
  \LeftLabel{\scriptsize$\mathbb{R} \text{ con } \ell = Q(f(c))$}
  \BinaryInfC{$R(c,s(c))\implies$}
  \AxiomC{$\implies R(c,s(c))$}
  \LeftLabel{\scriptsize$\mathbb{R}$}
  \BinaryInfC{$\implies$ ($\equiv \qedsymbol$)}
\end{prooftree}
\noindent
pertanto $(S/H')^*$ è insoddisfacibile. \\
Quindi $S/H_S$ è insoddisfacibile. \\
$S$ è insoddisfacibile ed il problema iniziale~(\ref{enuciato-in-sequenti}) era vero.

Tuttavia, già da questo esempio si può intuire che c'è qualche difetto: si vorrebbe considerare sottoinsiemi finiti $H \subseteq H_S$ utili, dai quali si generano solo le clausole che potrebbero essere utili, senza generare clausole inutili.

\paragraph{Esempio: ``paradosso'' dell'uomo col cappello}

Si asserisce che la seguente è una verità logica:
\begin{align*}
\models \exists x (P(x) \rightarrow \forall y P(y)) &&
\text{c'è un \textit{x} che se si mette il cappello,} \\ &&
\text{allora tutti gli altri \textit{y} lo mettono}
\end{align*}
Ma è davvero così? Se lo fosse, allora: 
$$
\neg (\exists x (P(x) \rightarrow \forall y P(y))) \text{ è insodd.}
$$
\begin{align*}
&\forall x \neg (P(x)  \rightarrow \forall y P(y)) \\
\equiv &\ \forall x \neg (\neg P(x) \lor \forall y P(y)) \\
\equiv &\ \forall x (P(x) \land \neg (\forall y P(y))) \\
\equiv &\ \forall x \exists y (P(x) \land \neg P(y)) \\
equisodd. &\ \forall x (P(x) \land \neg P(f(x))) & \text{ per Skolemizzazione}
\end{align*}

Che si traduce in:
$$
S = \{\implies P(x), P(f(x)) \implies\} \text{ è insodd.}
$$

con:
\begin{align*}
\mathscr{F}_S = \{f(\cdot), \mathcal{z}\} &&
H_S = \{\mathcal{z}, f(\mathcal{z}), f(f(\mathcal{z})), \cdots\, f^{(k)}(\cdots), \cdots\}
\end{align*}
Si cerca, quindi, un $H \subseteq_\omega H_S$ che dimostri l'isoddisfacibilità:
  \paragraph{(1)} $H = \{\mathcal{z}\}$: \\
  $$
  S = \{\implies P(\mathcal{z}), P(f(\mathcal{z})) \implies\}
  $$
  ma non si può fare nulla. 
  \paragraph{(2)} $H = \{\mathcal{z}, f(\mathcal{z})\}$:
  $$
  S = \{\implies P(\mathcal{z}), P(f(\mathcal{z})) \implies\} \cup  \{\implies P(f(\mathcal{z})), P(f(f((\mathcal{z}))) \implies\}
  $$
  A questo punto si può risolvere: 
  \begin{prooftree}
    \AxiomC{$P(f(\mathcal{z})) \implies$}
    \AxiomC{$\implies P(f(\mathcal{z}))$}
    \BinaryInfC{$\implies$}
  \end{prooftree}
Pertanto $S$ è insoddisfacibile, e di contro è vero che 
$$
\models \exists x (P(x) \rightarrow \forall y P(y))
$$
E ci sono due $\mathscr{L}$-strutture che la rendono vera:
\begin{itemize}
  \item dove esiste un $x \notin P$ (e perciò si può implicare tutto dal falso)
  \item dove ogni $x \in U$ è anche $x \in P$ (e perciò ogni elemento ``ha il cappello'')
\end{itemize}

\subsubsection{Difetti}
Sia 
$$
S = \{\implies P(x,y), Q(x)\}, \{P(x,f(x)) \implies \}
$$
si costruiscono 
\begin{align*}
\mathscr{F}_S = \{f(\cdot), \mathcal{z}\} &&
H_S=\{\mathcal{z}, f(\mathcal{z}), f(f(\mathcal{z})), \cdots \}
\end{align*}
Una possibile risoluzione si trova ponendo, ad esempio, $x = \mathcal{z}$, $y = f(\mathcal{z})$ 
\begin{prooftree}
  \AxiomC{$\implies P(\mathcal{z},f(\mathcal{z})), Q(\mathcal{z})$}
  \AxiomC{$P(\mathcal{z},f(\mathcal{z})) \implies$}
  \BinaryInfC{$\implies Q(\mathcal{z})$}
\end{prooftree}
Allo stesso modo se ne può ha un'altra con  $x = f(\mathcal{z})$, $y = f(f(\mathcal{z}))$: 
\begin{prooftree}
  \AxiomC{$\implies P(f(\mathcal{z}),f(f(\mathcal{z}))), Q(f(\mathcal{z}))$}
  \AxiomC{$P(f(\mathcal{z}),f(f(\mathcal{z})) \implies$}
  \BinaryInfC{$\implies Q(f(\mathcal{z}))$}
\end{prooftree}
e, continuando così, si possono ottenere infinite altre risoluzioni.

Per il nostro apparato concettuale, tutte queste risoluzioni sono semplici istanziazioni di variabili in termini ground, tuttavia noi umani vediamo qualcosa in più: vi è infatti un \textit{pattern} che ci piacerebbe catturare:
$$
y \mapsto f(x)
$$
questa è una sostituzione ma non è una istanziazione, in quanto $y$ non ``diventa'' un termine ground, ma possiede una variabile. Tuttavia, se si riesce a disegnare un quadro in cui questo è lecito, si può risolvere direttamente 
\begin{prooftree}
  \AxiomC{$\implies P(x,f(x)), Q(x)$}
  \AxiomC{$P(x,f(x)) \implies$}
  \AxiomC{$:y \mapsto f(x)$}
  \TrinaryInfC{$\implies Q(x)$}
\end{prooftree}
E da $\implies Q(x)$ si può ritrovare, con le istanziazioni delle variabili in termini ground, tutta la famiglia di risoluzioni.

\subsection{Teoria delle Sostituzioni e Unificazione}
Quali clausole del Primo Ordine generano per istanziazione la clausola vuota ($\implies$)? \\
Solo la clausola vuota stessa. L'approccio proposto poco fa, allora, è promettente, poiché: 
\begin{itemize}
  \item Ci sono delle sostituzioni \textit{ottime} che sono ``facili'' da calcolare e trovare 
   \item (``Lifting'') Facendo prima le risoluzioni a livello non ground si riesce a posticipare la fase d'istanziazione (ground) o, addirittura, non avere più bisogno di farla perchè si è trovata la clausola vuota in fase di \textit{lifting}
\end{itemize}

Abbiamo già discusso di sostituzioni riguardo la Logica Proposizionale, in cui 
si sostituivano delle lettere proposizionali con una Formula. Nella Logica dei Predicati, 
abbiamo discusso di sostituzione di variabili individuali con termini. Anche 
ora discuteremo di qualcosa di simile, ma ora la notazione cambia in quanto 
sarà proprio la sostituzione il punto focale del discorso. 

\begin{defi}[Sostituzione]
Una \textbf{sostituzione} $\sigma$ è una mappa 
$$
\sigma: Var \rightarrow \mathscr{T_L}
$$
con $\mathscr{T_L}$ che indica l'insieme degli $\mathscr{L}$-termini costruibili sul linguaggio $\mathscr{L}$, anche non ground. \
\end{defi}
\begin{defi}
Il dominio di una sostituzione $\sigma$ è l'insieme delle variabili $x$ che $\sigma$ \textit{non} manda in sé stessa: $\{x \in Var : x \neq \sigma(x)\}$
\end{defi}
\begin{pro}
Il dominio di $\sigma$ è \textit{finito}.
\begin{align*}
\sigma: x_1 \mapsto t_1, \cdots, x_k \mapsto t_k &&
dom(\sigma) = \{x_1, \cdots, x_k\}
\end{align*}
con $k \in \mathbb{N}$
\end{pro}
\noindent
Questa proprietà è importante dal punto di vista computazionale. \\
La notazione $t_i \mapsto x_i$ sostituisce la notazione $[t_i/x_i]$ utilizzata fino ad ora. 

\begin{defi}[Estensione canonica]
Una sostituzione $\sigma$ si estende canonicamente ad una mappa 
$$
\hat{\sigma}: \mathscr{T_L} \mapsto \mathscr{T_L}
$$
in modo induttivo sugli $\mathscr{L}$-termini:
\begin{itemize}
  \item \textbf{base}: $\hat{\sigma}(x) := \sigma(x)$ per ogni $x \in Var$
  \item \textbf{passo}: $\hat{\sigma}(f(t_1, \cdots, t_n)) := f(\hat{\sigma}(t_1), \cdots, \hat{\sigma}(t_n))$
\end{itemize}
Si può, inoltre, estendere $\sigma$ non solo ai termini ma anche alle espressioni/formule $E$:
\begin{itemize}
  \item \textbf{base}: se $E = P(t_1, \cdots, t_n)$, allora:
  $$
  \hat{\sigma}(P(t_1, \cdots, t_n)) := P(\hat{\sigma}(t_1), \cdots, \hat{\sigma}(t_n))
  $$
  \item \textbf{passo}: se $E$ è una espressione generica, allora $  \hat{\sigma}(E)$ è l'espressione ottenuta sostituendo tramite $\sigma$ simultaneamente tutte le occorrenze dei termini $x_i$ con $\sigma(x_i)$
\end{itemize}
\end{defi}

Scriveremo spesso $E\sigma$ per indicare $\hat{\sigma}(E)$.

\subsubsection{Proprietà delle Sostituzioni}
\begin{itemize}
  \item{\textbf{Composizione}} Se $\gamma, \tau : Var \rightarrow \mathscr{T_L}$ la sostituzione composta $\sigma\tau$ è data da
    $$
    \sigma\tau(x) = \hat{\tau}(\sigma(x))
    $$
    Ovviamente $\hat{\sigma\tau}$ applicata su una certa espressione $E$, denotato quindi $E\sigma\tau$ si ottiene applicando $\hat{\tau}(\sigma(x_i))$ ad ogni variabile occorrente in $E$.
    \item{\textbf{Sostituzione Identica}} la sostituzione $id$ è definita 
    $$
    id(x_i) = x_i ~~~ \text{ per ogni } x_i \in Var
    $$
    e ha dominio vuoto ($dom(id) = \emptyset$); gode della proprietà di composizione seguente:
    $$
    \sigma id = id \sigma = \sigma ~~~ \text{ per ogni } \sigma:Var \rightarrow \mathscr{T_L}
    $$
    quindi svolge il ruolo di elemento neutro.
  \item{\textbf{Rinomine}} una rinomina è una sostituzione $\rho: Var \rightarrow \mathscr{T_L}$ che è una permutazione del suo dominio, ossia 
    \begin{align*}
    \rho(x_i) = x_j && \text{con $\rho$ biettiva sul suo dominio}
    \end{align*}
    quindi $\rho$ scambia un set di variabili. \\
    Inoltre ogni rinomina è invertibile, cioè esiste una sostituzione $\rho'$ tale che $\rho'(\rho(x_i)) = x_i$, in altre parole $\rho\rho' = \rho'\rho = id$.
  \item{\textbf{Sostituzione Generale}} $\sigma$ è detta \textit{più generale} di $\tau$ se esiste una sostituzione $\delta$ tale che $\sigma\delta = \tau$. \\ 
    Per esempio, se
    \begin{align*}
    \sigma: &\ x \mapsto f(z),\ y \mapsto z \\
    \tau : &\  x \mapsto f(z),\ y \mapsto c,\ z \mapsto c
    \end{align*}
    allora $\sigma$ è più generale di $\tau$ ($\sigma \geq \tau$) perché $\tau = \sigma\delta$ con $\delta: z \mapsto c$:
    \begin{align*}
    \sigma\delta: &\ x \underbrace{\stackrel \sigma \mapsto f(z) \stackrel \delta \mapsto}_{\tau} f(c) \\
    \sigma\delta: &\ y \underbrace{\stackrel \sigma \mapsto z \stackrel \delta \mapsto}_{\tau} c \\
    \sigma\delta: &\ z \underbrace{\stackrel \sigma \mapsto z \stackrel \delta \mapsto}_{\tau} c
    \end{align*}
  \item{\textbf{Preordine}} La relazione ``essere più generale'' ($\sigma \geq \tau$) è riflessiva e transitiva, ma in generale non è antisimmetrica. Per questo non può essere una relazione d'ordine, ma di \textit{preordine}. \\
    Non è antisimmetrica perché è possibile che $\sigma \neq \tau$ anche se $\sigma \geq \tau$ e al contempo $\tau \geq \sigma$, perché hanno sostituzioni distinte. Queste sostituzioni se sono distinte, tuttavia, sono solo rinomine $\rho_1$, $\rho_2$ tali che 
    $$
    \sigma\rho_1 = \tau ~~~~~ \tau\rho_2 = \sigma
    $$
  \item{\textbf{Equivalenza Sostituzionale}} Se $\sigma \geq \tau$ e $\tau \geq \sigma$, dichiariamo che $\sigma \equiv \tau$.

  \item{\textbf{Ordine Parziale}} L'insieme delle sostituzioni con la relazione ``essere più generale'' quozientato rispetto a $\equiv$ è un \textit{poset}, ossia
  $$
  \{[\sigma]_{\equiv}: \sigma : Var \rightarrow \mathscr{T_L}\} 
  $$
  è un insieme con ordine parziale rispetto a $\geq$.
\end{itemize}

L'ultimo concetto che ci serve per arrivare alla definizione della Risoluzione 
Sollevata o Liftata è il seguente: 
\begin{defi}[Unificazione]
        Un problema di Unificazione è una lista finita di coppie di termini 
        $$
        t_1 \stackrel{?}{=} u_1, \cdots, t_n \stackrel{?}{=} u_n
        $$
\end{defi}
La soluzione ad un problema di unificazione è una sostituzione che verifica tutte 
le uguaglianze nella lista di coppie di termini, ossia una sostituzione $\sigma: Var \rightarrow \mathscr{T_L}$ tale che:
$$
\hat{\sigma}(t_1) = \hat{\sigma}(u_1), \cdots, \hat{\sigma}(t_n) = \hat{\sigma}(u_n)
$$
\begin{defi}[Unificatore]
  Una soluzione $\mu: Var \rightarrow \mathscr{T_L}$ è detta \textbf{unificatore} di massima generalità (\textit{most general unifier}) se e solo se $\mu$ è più generale di ogni altra soluzione al problema di unificazione.
\end{defi}

\begin{teo}[di Unificazione]
Ogni problema di unificazione $U$ o ammette una soluzione $\mu$ che è un MGU—i.e., Most General Unifier—oppure non ammette alcuna soluzione.
\end{teo}
Si noti che se $mgu(U)$ esiste non è necessariamente unico, in quanto è un unica classe di equivalenza $mgu(U) = [\mu]_{\equiv}$.

Esistono algoritmi che, su input un problema di unificazione $U$, trovano velocemente $\mu = mgu(U)$ se esiste, e si fermano se $mgu(U)$ non esiste.

Ci interesserà risolvere problemi del tipo 
$$
P(t_1, \cdots, t_m) \stackrel{?}{=} Q(s_1, \cdots, s_n)
$$
per $P$ simbolo di predicato $m$-ario e $Q$ simbolo di predicato $n$-ario. In 
altre parole ci interessa unificare due atomiche. Si noti che il problema appena 
esposto è unificabile se $P = Q$ e dunque $m = n$. Il problema si riduce a 
$$
P(t_1, \cdots, t_n) \stackrel{?}{=} P(s_1, \cdots, s_n)
$$
che a sua volta si riduce a 
$$
U: t_1 \stackrel{?}{=}s_1, \cdots, t_n \stackrel{?}{=}s_n
$$

\subsection{Calcolo $\mathscr{R}$ della risoluzione liftata}
Si supponga di avere le seguenti regole scritte in forma 
a sequenti: 
$$
\{\{\Gamma \implies \Delta, A\}, \{B, \Gamma' \implies \Delta'\}\}
$$
dove le variabili occorrenti in $\Gamma \implies \Delta, A$ sono disgiunte da quelle in $B, \Gamma' \implies \Delta'$ (il che si può sempre rispettare usando le rinomine). 

$A$ e $B$ sono proposizioni atomiche, mentre $\Gamma, \Delta, \Gamma', \Delta'$ sono insiemi di proposizioni atomiche.

Se si riesce a calcolare un $\mu = mgu(A,B)$, allora si può risolvere la \textbf{risoluzione liftata} su $\mathbb{R}(\mu(A), \mu(B))$: 
\begin{prooftree}
        \AxiomC{$\Gamma \implies \Delta, A$}
        \AxiomC{$B, \Gamma' \implies \Delta'$}
        \BinaryInfC{$\mu(\Gamma), \mu(\Gamma'), \implies \mu(\Delta), \mu(\Delta')$}
\end{prooftree}

Questa regola incarna il calcolo $\mathscr{R}$. Da sola non basta a garantire la completezza 
refutazionale, ma è necessario aggiungere almeno una delle seguenti due 
regole di fattorizzazione: 
\begin{itemize}
  \item \textbf{Fattorizzazione Destra}: se si riesce a calcolare un $\mu = mgu(A,B)$, allora si può unificare $A$ e $B$ tenendo un'unica copia dei due:
    \begin{prooftree}
      \AxiomC{$\Gamma \implies \Delta, A, B$}
      \RightLabel{\scriptsize$\mu = mgu(A,B)$}
      \UnaryInfC{$\Gamma\mu \implies \Delta\mu, A\mu$}
    \end{prooftree}
  \item \textbf{Fattorizzazione Sinistra}: uguale, ma lavorando sulla parte a sinistra di $\implies$
    \begin{prooftree}
      \AxiomC{$\Gamma, A, B \implies \Delta$}
      \RightLabel{\scriptsize$\mu = mgu(A,B)$}
      \UnaryInfC{$\Gamma\mu, A\mu \implies \Delta\mu$}
    \end{prooftree}
\end{itemize}

Una refutazione di un insieme di clausole $S$ nel calcolo $\mathscr{R}$ è una successione di clausole $C_1, C_2, \cdots, C_u$ tale che: 
\begin{enumerate}
  \item $C_u = \implies$ (clausola vuota)
  \item per ogni $C_i$:
    \begin{itemize}
      \item o $C_i$ è un membro di $S$, eventualmente rinominato
      \item o $C_i$ è ottenuto da eventuali rinomine di $C_j, C_k$ con $(j,k < i)$ per risoluzione liftata o fattorizzazione (destra o sinistra)
    \end{itemize}
\end{enumerate}
Il calcolo $\mathscr{R}$ è \textit{refutazionalmente completo} per ogni insieme di clausole $S$, quindi anche per tutta la logica del Primo Ordine: se si ha un problema del tipo
$$
\Gamma \stackrel{?}{\models} A \leadsto \stackrel {\text{insieme di clausole del Primo Ordine}} {S \text{ insod.?}} \leadsto S \stackrel ? {\vdash_\mathbb{R}} \implies
$$
Questo ci da, quindi:
\begin{itemize}
  \item \textit{completezza}: Se $S \vdash_R \implies$, allora $S$ è insodd. e $\Gamma \models A$
  \item \textit{correttezza}: se $\Gamma \models A$, allora $S \vdash_R \implies$; \\
  ovvero $S$ è insodd. e pertanto deve essere refutabile col calcolo $\mathscr{R}$
\end{itemize} 


\subsubsection{Considerazioni Finali}
La completezza refutazionale di $\mathbb{R}$ mostra che la logica del Primo Ordine è \textit{almeno} semidecidibile. ``Almeno'' in quanto il Teorema di Church afferma che non si può fare di meglio, ossia la Logica del Primo Ordine è semidecidibile ma non decidibile. 

Siano $\Gamma \models A$ e $S$ l'insieme, anche infinito, derivato di clausole; con $H_S$ l'universo di Herbrand relativo.
Sia 
$$
S_1 \subseteq_{\omega} S_2 \subseteq_{\omega} \cdots \subseteq_{\omega}S
$$
un insieme di sottoinsiemi finiti di $S$ tali che $\bigcup_{i \in \omega} S_i = S$ e per ogni $i \in \omega$ sia $H^i$ l'universo di Herbrand di $S_i$. Chiaramente, si ha anche che 
$$
H^1 \subseteq H^2 \subseteq \cdots \subseteq H_s
$$
e $\bigcup_{i \in \omega} H^i = H_s$. Per ogni $i$ si definisce 
$$
H_1^i \subseteq_{\omega} H_2^i \subseteq_{\omega}\cdots \subseteq_{\omega} H^i
$$
Per ogni $k \in \omega$ e per ogni $i,j$ tale che $i+j = k$ si applica DPP o DPLL a $S_i/H_j^i$. 
Se si trova la clausola vuota, allora $S$ è insoddisfacibile. Se non si trova, 
allora si passa a $k=k+1$. In questo modo si riescono a perlustrare tutti i sottoinsiemi 
finiti dello spazio di ricerca, e se $S$ è insoddisfacibile prima o poi si trova. 

La morale sottesa a questa considerazione è che se $S$ è insoddisfacibile 
prima o poi si mostra che lo è. Se $S$ è soddisfacibile non ci sono criteri generali 
per fermare l'algoritmo. 


\begin{defi}[Completezza Formale]
        Una teoria $\Gamma$ si dice Formalmente Completa se e solo se per 
        ogni enunciato $A$ è vero o $\Gamma \vdash A$ oppure $\Gamma \vdash \neg A$. 
        Per Completezza e Correttezza Semantica, questo equivale a dire 
        che $\Gamma \models A$ oppure $\Gamma \models \neg A$.
\end{defi}

\begin{teo}[Teorema di Incompletezza di G\"odel]
        Se $T$ è una teoria assiomatizzabile, consistente e che esprime 
        l'Aritmetica di Peano, allora esiste $\phi$ tale che 
        $$
        T \nvdash \phi ~~~~~ T \nvdash \neg \phi
        $$
        ma $\mathscr{N} \models \phi$ con $\mathscr{N} = (\mathbb{N}, +, \times, 0, s)$. Quindi è formalmente incompleta.
\end{teo}

\begin{defi}[Aritmetica di Presburger]
        Sostanzialmente, è l'Aritmetica di Peano ($PrA$) senza moltiplicazione. \\
        $PrA$ è formalmente completa—e quindi anche decidibile—in tempo doppiamente esponenziale.
\end{defi}

\begin{defi}[Aritmetica di Robinson]
  Sostanzialmente, è l'Aritmetica di Peano senza schema di induzione, più un assioma che sostituisce alcune particolarità dello schema di induzione:
  $$
  \forall x (\neg (x= 0) \rightarrow \exists y (s(y) = x))
  $$
  Si può dimostrare che, pur senza induzione, è anch'essa formalmente incompleta, così come tutte le sue estensioni consistenti e assiomatizzabili. Perciò anche PA stessa. \\
  Infatti RA—come PA—è semidecidibile ma non decidibile, tuttavia RA è finitamente assiomatizzabile: ha solo 7 assiomi.
\end{defi}

\begin{teo}[Teorema di Church]
  La Logica del Primo Ordine è indecidibile
\end{teo}
\begin{proof}
  Con $RA = \{A_1, A_2, \cdots, A_7\}$ sia C un enunciato nell'Aritmetica di Robinson stessa
  $$
  C = A_1 \land \cdots \land A_7
  $$
  e $A$ un enunciato arbitrariamente scelto. \\
  Si suppone, per assurdo, che la Logica del Primo Ordine sia decidibile. Allora:
  \begin{itemize}
    \item $C \rightarrow A$ è un enunciato del Primo Ordine, 
    \item dunque è decidibile se $\models C \rightarrow A$ oppure $\nvDash C \rightarrow A$. 
      \begin{itemize}
        \item se fosse $\models C\rightarrow A$, allora avremmo 
          \begin{prooftree}
            \AxiomC{$C$}
            \AxiomC{$C \rightarrow A$}
            \RightLabel{\scriptsize{Modus Ponens}}
            \BinaryInfC{$A$}
          \end{prooftree}
          e che quindi $RA \models A$
        \item se, invece, fosse $\nvDash C \rightarrow A$, allora per il teorema di deduzione $C \nvDash A$, dunque $RA \nvDash A$
      \end{itemize}
  \end{itemize}
  Dunque, abbiamo deciso se $RA \models A$ oppure $RA \nvDash A$, cioè: $RA$ è decidibile. \\ Assurdo. Pertanto, la Logica del Primo Ordine non è decidibile.
\end{proof}

Mentre l'insieme degli enunciati insoddisfacibili è enumerabile, l'insieme degli enunciati soddisfacibili non è enumerabile, poiché se così fosse ci sarebbe un modo per rendere decidibile una qualunque aritmetica, che è assurdo.
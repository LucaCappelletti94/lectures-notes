\providecommand{\main}{..}
\documentclass[\main/main.tex]{subfiles}
\begin{document}
\chapter{Crawling}
\section{Politeleness (buona educazione)}
Si riferisce, nel crawling, ad un insieme di strategie utilizzate per evitare di essere bloccati da siti e provider.

Questa deve essere vincolata per \textbf{host} e per \textbf{ip}, e consiste nell'evitare di sovraccaricare la macchina che offre il servizio per esempio facendo una pausa tra una richiesta e quella successiva, magari proporzionale al tempo di scaricamento.

È altamente suggerito cercare di seguire le regole di crawling che ogni sito esplicita nel proprio robots.txt.

\subsection{Struttura dati degli url nel crawler: coda con ritardo}
La struttura è composta da una \textbf{coda di siti con priorità} e per ogni sito una \textbf{coda di url}. Ogni sito avrà associato un timestamp, rappresentante il tempo dopo il quale si potrà nuovamente fare richieste al sito rispettando la politeleness.

Comprendendo anche gli \textbf{IP} nella coda, si procede ad aggiungere un nuovo primo livello alla coda, che ora diviene una coda di ip con associate code di host a cui a loro volta vengono associate code di url.

\section{DNS}

\end{document}
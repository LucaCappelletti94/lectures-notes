\providecommand{\main}{../../..}
\documentclass[\main/main.tex]{subfiles}
\begin{document}
\chapter{Metodi di ottimizzazione}
\section{Metodi a discesa rapida}
Son metodi che usano come funzione di aggiornamento \(\bmx_{k+1} = \bmx_k - \a_k\nabla f_k\), in cui le direzioni sono ortogonali al contorno della funzione. Non dovendo calcolare l'hessiana lo sforzo computazionale non è eccessivo e converge globalmente, ma estremamente piano quando una funzione è patologica.

\begin{theorem}[Velocità di convergenza locale dei metodi a discesa rapida]
    Data una matrice \(Q\) definita positiva, la seguente relazione vale \(\forall \bmx \in \R^n \):
    \[
        \frac{
            (\bmxt \bmx)^2
        }{
            (\bmxt Q \bmx)(\bmxt Q^-1 \bmx)
        }
        \geq
        \frac{4\lambda_{\min}\lambda_{\max}}{(\lambda_{\min} + \lambda_{\max})^2}
    \]
    Dove \(\lambda_{\min}\) e \(\lambda_{\max}\) son gli autovalori mimimo e massimo di \(Q\).

    Ne segue che la velocità di covergenza dei metodi a discesa rapida è \textbf{lineare} per i modelli quadratici.

    \[
        \frac{
            \norm{\bmx_{k+1}-\bmx^*}_Q
        }{
            \norm{\bmx_{k}-\bmx^*}_Q
        }
        \leq
        \frac{
            (\lambda_{\max} - \lambda_{\min})
        }{
            (\lambda_{\max} + \lambda_{\min})
        }
    \]

\end{theorem}

\section{Metodi Newton}
Prendendo in considerazione l'approssimazione di Taylor fermata al secondo ordine, quindi con l'hessiana, otteniamo come direzione di discesa quella che minimizza \(\nabla f^T \bmd + \frac{1}{2}\bmdt H\bmd \), cioè \(\bmd = {-(H)}^{-1} \nabla f\). Quando \(H\) è \textbf{definita positiva} vale che:

\[
    \nabla f^T \bmd = -\bmd H \bmd \leq -\sigma \norm{\bmd}^2
\]

Cioè quando \(H\) è \textbf{definita positiva} la direzione di Newton è la \textbf{direzione di discesa}.

Nei \textbf{modelli quadratici} con \(Q\) \textbf{definita positiva} il metodo di Newton converge in un'iterazione, altrimenti non converge. Su funzioni generiche, la qualità della direzione dipende da quanto è definita positiva la matrice hessiana.

\begin{theorem}[Convergenza dei metodi di Newton]
    Sia \(f \in C^2\) e sia \(H(x)\) continuamente lipschitziana in un intorno del punto ottimo \(\bmx^*\). Si assuma che valga \(\bmx_{k+1} = \bmx_k + \bmd_k\). Allora:
    \begin{enumerate}
        \item Se \(\bmx_0\) è sufficientemente vicino a \(\bmx^*\), allora \(\crl{\bmx_k} \rightarrow \bmx^*\)
        \item \(\crl{\bmx_k}\) converge \textbf{quadraticamente}
        \item \(\crl{\norm{\nabla f(\bmx_k)}}\) converge quadraticamente a zero.
    \end{enumerate}
    I metodi Newton sono \textbf{convergenti localmente}.
\end{theorem}

La complessità computazionale è di \(O(n^3)\)

\subsection{Metodi Newton modificati}
Sono metodi che modificano l'Hessiano, o rendendo la matrice positiva o scegliendo una direzione di discesa tramite metodi di discesa rapida quando necessario.


\section{Metodi Quasi-Newton}
Sono metodi in cui viene utilizzata un'approssimazione dell'Hessiano, che è computazionalmente costoso da calcolare. Viene utilizzata al suo posto una matrice chiamata \(G_k\) al posto di \(H_k^{-1}\), e quindi calcolano la direzione di discesa come \(\bmd_k = -G_k \nabla f(\bmx_k)\).

\begin{definition}[Relazione secante]
    Definendo \(\bmp_k = \nabla f(\bmx_{k+1}) - \nabla f(\bmx_k)\) possiamo definire la relazione secante:

    \[
        H(\bmx_k)\bmh_k \approx \bmp_k \qquad \text{or} \qquad {H(\bmx_k)}^-1\bmp_k \approx \bmh_k
    \]
\end{definition}

Inizializzando \(G_0 = I\), possiamo imporre che ad ogni iterazione la matrice \(G_{k+1}\) debba soddisfare la relazione secante con la seguente uguaglianza:

\[
    G_{k+1}\bmp_k = \bmh_k
\]

La realizzazione specifica di come si ottiene \(G_{k+1}\) partendo da \(G_k\) varia dai differenti metodi Quasi-Newton. Questi metodi impongono che \(G_k = G_k^T\) e che \(G_{k+1} - G_k\) abbia un rango basso.

Ne esistono di due categorie:

\begin{enumerate}
    \item Matrice a rango unitario simmetrico o SR1.
    \item A rango due:
          \begin{enumerate}
              \item DFP
              \item BFGS
          \end{enumerate}
\end{enumerate}

I \textbf{metodi a rango due} hanno alcune proprietà interessanti:

\begin{enumerate}
    \item La matrice \(G_k\) converge a \(H(\bmx_k)^-1\) sui modelli quadratici.
    \item Se \(G_0\) è definita positiva allora tutte le \(G_k\) sono definite positive.
    \item Il costo computazionale è di \(O(n^2)\) in ogni iterazione.
    \item La velocità di convergenza è \textbf{superlineare}.
    \item In particolare il metodo BFGS garantisce convergenza globale se lo step-size rispetta le condizioni di Wolfe.
\end{enumerate}

\section{Metodi del gradiente coniugato}
Si tratta di una delle tecniche più utili per risolvere sistemi lineari di grandi dimensioni e può essere adattata per risolvere problemi di ottimizzazione non lineare. Si tratta di un metodo iterativo che risolve sistemi della forma \(A\bmx = \bmb \) dove \(A\) è una matrice definita positiva che è \(A=A^T\).

\begin{definition}[Vettori coniugati]
    Un insieme di vettori \(\bmp_0, \bmp_1, \ldots, \bmp_h\) è detto \textbf{coniugato} rispetto ad una matrice \(A\) se vale la relazione:

    \[
        \bmp_i^T A \bmp_j = 0 \quad \forall i \neq j
    \]
\end{definition}

Dato un punto di inizio \(\bmx_0\) e le \textbf{direzioni coniugate} \(\bmp_0, \ldots, \bmp_{n-1}\) generiamo la sequenza \(\bmx_{k+1} = \bmx_k + \a_k\bmp_k\) dove \(\a_k\) è un minimizzatore monodimensionale ed è dato da:

\[
    \a_k = - \frac{r_k^T\bmp_k}{\bmp_k^T A \bmp_k}
\]

\begin{theorem}{Convergenza di sequenza con vettori coniugati}
    Dato un qualsiasi \(\bmx_0\), la sequenza generata da \(\bmx_{k+1} = \bmx_k + \a_k\bmp_k\) converge a \(\bmx^*\) in al più \(n\) iterazioni.
\end{theorem}

Il metodo del gradiente coniugato è un metodo a direzione coniugata con una proprietà importante: \(\bmp_k\) può essere ottenuta sapendo solo \(\bmp_{k-1}\) e \(\bmp_k\) risulta coniugato a tutte le direzioni precedenti.

Il vettore \(\bmp_k\) è definito come una combinazione lineare del gradiente e del vettore precedente:

\[
    \bmp_k = -\nabla f_k + \beta_k\bmp_{k-1}
\]
Dove \(\beta \) è definito sfruttando la definizione di vettore coniugato:

\[
    \beta_k = \frac{\nabla f_k^T A \bmp_{k-1}}{\bmp^T_{k+1} A \bmp_{k-1}}
\]

Inizialmente il valore del vettore è \(\bmp_0 = -\nabla f_0\).

Esistono vari metodi del gradiente coniugato che variano in base a come viene calcolato il valore di \(\beta_{k+1}\) a partire dall'iterazione precedente.

In particolare è utile perchè utilizza poco spazio, non è computazionalmente pesante e converge rapidamente. Viene però preferita l'eliminazione gaussiana quando il problema è di piccola dimensione, essendo meno sensibile agli errori di arrotondamento.

\end{document}